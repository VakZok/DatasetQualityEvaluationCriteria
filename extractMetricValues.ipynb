{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Imports"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-01-21T21:46:23.343386Z",
     "start_time": "2024-01-21T21:46:23.335590Z"
    }
   },
   "outputs": [],
   "source": [
    "import glob\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import evaluate\n",
    "from scipy.stats import kstest"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Prepare function to calculate autocorrelation"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def autocorrelation_per_instance(series):\n",
    "    \"\"\"Calculates autocorrelation for a given time series.\"\"\"\n",
    "    n = len(series)\n",
    "    if n < 2:  # If there's only one data point, autocorrelation is not defined\n",
    "        return None\n",
    "    return series.autocorr()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-21T21:46:23.343679Z",
     "start_time": "2024-01-21T21:46:23.339456Z"
    }
   },
   "execution_count": 16
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Generating statistics"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Setup folder path for the UCR datasets (TSV files)\n",
    "prepared_datasets_folder_path = \"Prepared_UCR_Datasets\"\n",
    "unconcatenated_datasets_folder_path = \"UCR_Datasets\"\n",
    "\n",
    "# Data sets with varying length\n",
    "varying_length_datasets = [\"AllGestureWiimoteX\", \"AllGestureWiimoteY\", \"AllGestureWiimoteZ\", \"GestureMidAirD1\", \"GestureMidAirD2\", \"GestureMidAirD3\", \"GesturePebbleZ1\", \"GesturePebbleZ2\",\n",
    "\"PickupGestureWiimoteZ\", \"PLAID\", \"ShakeGestureWiimoteZ\"]\n",
    "\n",
    "# Use glob to get a list of TSV file paths in the folder\n",
    "tsv_files = glob.glob(os.path.join(prepared_datasets_folder_path, '*.tsv'))\n",
    "\n",
    "# Initialize an empty DataFrame\n",
    "columns = ['dataset_name', 'num_instances_train', 'num_instances_test', 'train_test_ratio', 'num_time_points', 'num_classes', 'class_distribution_bias', 'default_rate', 'num_missing_values', 'null_rate', 'num_duplicate_instances', 'data_repetition_rate', 'avg_autocorrelation', 'ks_statistic', 'ks_p_value']\n",
    "\n",
    "overview_df = pd.DataFrame(columns=columns)\n",
    "\n",
    "# Preparing the Huggingface Evaluation of label distribution\n",
    "distribution = evaluate.load(\"label_distribution\")\n",
    "\n",
    "# Loop through each TSV file and read it into a DataFrame - append the name of the file\n",
    "for tsv_file in tsv_files:\n",
    "    # Read the TSV file into a DataFrame\n",
    "    df = pd.read_csv(tsv_file, sep='\\t', header=None)\n",
    "\n",
    "    # Extract the dataset name from the file path\n",
    "    dataset_name = os.path.splitext(os.path.basename(tsv_file))[0]\n",
    "\n",
    "    # Initialize an empty dictionary to store the statistical information\n",
    "    stat_info = {}\n",
    "    \n",
    "    # Append number of instances in train and test set, train-test ratio\n",
    "    try:\n",
    "        # Paths for TRAIN and TEST datasets\n",
    "        train_file_path = os.path.join(unconcatenated_datasets_folder_path, f\"{dataset_name}_TRAIN.tsv\")\n",
    "        test_file_path = os.path.join(unconcatenated_datasets_folder_path, f\"{dataset_name}_TEST.tsv\")\n",
    "\n",
    "        # Read TRAIN and TEST datasets\n",
    "        train_df = pd.read_csv(train_file_path, sep='\\t', header=None)\n",
    "        test_df = pd.read_csv(test_file_path, sep='\\t', header=None)\n",
    "\n",
    "        # Number of instances in TRAIN and TEST datasets\n",
    "        # -1 if dataset has varying length (drawn from comparing with values at UCR Archive website)\n",
    "        if dataset_name in varying_length_datasets:\n",
    "            stat_info['num_instances_train'] = train_df.shape[0] - 1\n",
    "            stat_info['num_instances_test'] = test_df.shape[0] - 1\n",
    "        else:\n",
    "            stat_info['num_instances_train'] = train_df.shape[0]\n",
    "            stat_info['num_instances_test'] = test_df.shape[0]\n",
    "\n",
    "        # Calculate train-test ratio\n",
    "        stat_info['train_test_ratio'] = round(stat_info['num_instances_train'] / stat_info['num_instances_test'], 2)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error while reading TRAIN/TEST files for {dataset_name}: {str(e)}\")\n",
    "    \n",
    "    # Append basic statistical information\n",
    "    try:\n",
    "        # dataset name\n",
    "        stat_info['dataset_name'] = dataset_name\n",
    "        \n",
    "        # number of features\n",
    "        stat_info['num_time_points'] = df.shape[1] - 1\n",
    "        \n",
    "        # number of classes\n",
    "        # -1 if dataset has varying length (drawn from comparing with values at UCR Archive website)\n",
    "        if dataset_name in varying_length_datasets:\n",
    "            stat_info['num_classes'] = len(df.iloc[1:, 0].unique()) - 1\n",
    "        else:\n",
    "            stat_info['num_classes'] = len(df.iloc[:, 0].unique())\n",
    "        \n",
    "        # first sum: sum of missing values per column, second sum: sum of all columns\n",
    "        stat_info['num_missing_values'] = df.isnull().sum().sum() \n",
    "        \n",
    "        # percentage of missing values\n",
    "        # total values (exlude label column) - missing values / total values\n",
    "        total_values = df.shape[0] * df.shape[1] - df.shape[0]\n",
    "        stat_info['null_rate'] = round(stat_info['num_missing_values'] / total_values, 4)\n",
    "        \n",
    "        # number of duplicate instances\n",
    "        # keep=False marks all duplicates as True, sum() counts the Trues\n",
    "        stat_info['num_duplicate_instances'] = df.duplicated().sum()\n",
    "        \n",
    "        # percentage of duplicate instances\n",
    "        total_instances = df.shape[0]\n",
    "        stat_info['data_repetition_rate'] = round(stat_info['num_duplicate_instances'] / total_instances, 4)\n",
    "    except Exception as e:\n",
    "        print(f\"Error while calculating basic statistical information for {str(dataset_name)}: {str(e)}\")\n",
    "\n",
    "    # Append label skew information\n",
    "    try:\n",
    "        # get values of the first column (class labels)\n",
    "        label_values = df.iloc[:, 0].values\n",
    "        result = distribution.compute(data=label_values)\n",
    "\n",
    "        # Append label skew information\n",
    "        stat_info['class_distribution_bias'] = round(result['label_skew'], 4)\n",
    "    except Exception as e:\n",
    "        print(f\"Error while calculating class imbalance for {str(dataset_name)}: {str(e)}\")\n",
    "        \n",
    "    # Append default rate\n",
    "    try:\n",
    "        # Total number of instances\n",
    "        total_instances = df.shape[0]\n",
    "        # Most common class\n",
    "        most_common_class = df[df.columns[0]].value_counts().idxmax()\n",
    "        # Count of instances in the most common class\n",
    "        count_most_common_class = df[df[df.columns[0]] == most_common_class].shape[0]\n",
    "        # Default error rate: (Total instances - Instances of most common class) / Total instances\n",
    "        stat_info['default_rate'] = round((count_most_common_class / total_instances), 4)\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error while calculating default rate for {dataset_name}: {str(e)}\")\n",
    "    \n",
    "    # Append autocorrelation information with try-except blocks\n",
    "    try:\n",
    "        time_series_data = df.iloc[:, 1:]  # Remaining columns as time series data\n",
    "        autocorrelations = time_series_data.apply(autocorrelation_per_instance, axis=1)\n",
    "        \n",
    "        stat_info['avg_autocorrelation'] = round(autocorrelations.mean(), 4)\n",
    "    except Exception as e:\n",
    "        print(f\"Error while calculating autocorrelation for {str(dataset_name)}: {str(e)}\")\n",
    "\n",
    "    # Perform KS test against a normal distribution\n",
    "    try:\n",
    "        # Flatten the dataset (excluding the label column)\n",
    "        flattened_data = df.iloc[:, 1:].values.flatten()\n",
    "    \n",
    "        # Generating a normal distribution with the same mean and std as the dataset\n",
    "        norm_dist = np.random.normal(np.mean(flattened_data), np.std(flattened_data), len(flattened_data))\n",
    "\n",
    "        # KS test\n",
    "        ks_statistic, ks_p_value = kstest(flattened_data, norm_dist)\n",
    "\n",
    "        # Add KS statistic and p-value to stat_info\n",
    "        stat_info['ks_statistic'] = round(ks_statistic, 4) # round to 4 digits to avoid scientific notation\n",
    "        stat_info['ks_p_value'] = round(ks_p_value, 4)\n",
    "    except Exception as e:\n",
    "        print(f\"Error while calculating KS statistic for {str(dataset_name)}: {str(e)}\")\n",
    "    \n",
    "    # Turn the dictionary into a DataFrame\n",
    "    stat_info_df = pd.DataFrame([stat_info])\n",
    "    \n",
    "    # Append the statistical information to the DataFrame\n",
    "    overview_df = pd.concat([overview_df, stat_info_df], ignore_index=True)\n",
    "    \n",
    "    print(f\"Finished calculating statistics for {str(dataset_name)}\")\n",
    "    \n",
    "overview_df.to_csv('extractedMetrics.csv', index=False)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-21T21:49:00.455136Z",
     "start_time": "2024-01-21T21:49:00.452448Z"
    }
   },
   "execution_count": 17
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
